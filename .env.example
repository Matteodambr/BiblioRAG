# BiblioRAG Configuration
# Copy this file to .env and fill in your credentials

# Mendeley API Credentials
# Get these from https://dev.mendeley.com/
MENDELEY_CLIENT_ID=your_client_id_here
MENDELEY_CLIENT_SECRET=your_client_secret_here

# Google Gemini API Key
# Get this from https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# LLM Provider Configuration
# Provider: "gemini" for Google Gemini API (cloud, requires API key), "ollama" for local Ollama
# Default: gemini
#LLM_PROVIDER=gemini

# LLM Model Configuration
# For Gemini (when LLM_PROVIDER=gemini): "gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash"
# For Ollama (when LLM_PROVIDER=ollama): "deepseek-r1:8b", "llama3.2:3b", "phi4:14b", etc.
GEMINI_MODEL=gemini-2.5-flash
#OLLAMA_LLM_MODEL=deepseek-r1:8b

# Enrichment Model for Document Indexing
# Use a local model during indexing to avoid API rate limits
# Examples: "ollama/llama3.2:1b", "ollama/qwen2.5:0.5b"
# Leave empty to use main LLM model for both indexing and queries
#ENRICHMENT_MODEL=ollama/llama3.2:1b

# Fallback Model for Rate Limit Protection (Gemini only)
# Automatically fall back to this model when Gemini hits rate limits
# Only used when LLM_PROVIDER=gemini. Leave empty to disable fallback
#FALLBACK_MODEL=ollama/deepseek-r1:8b

# Embedding Configuration
# Provider: "ollama" for local Ollama embeddings (default), "google" for Google embeddings
EMBEDDING_PROVIDER=ollama

# Embedding model name:
# - For Ollama: "nomic-embed-text" (default), "mxbai-embed-large", etc.
# - For Google: "models/embedding-001"
EMBEDDING_MODEL=nomic-embed-text

# Ollama server URL (only needed if using Ollama and not running on default port)
# OLLAMA_URL=http://localhost:11434

# Document Chunking Settings
# Split large PDFs (e.g., textbooks) into chapters for better indexing and retrieval
PAPERQA_SPLIT_LARGE_PDFS=true
# PDFs with this many pages or more are considered "large" and will be split
PAPERQA_LARGE_PDF_PAGES=100
# Fallback chunk size (pages) if PDF has no outline/table of contents
PAPERQA_CHUNK_SIZE_PAGES=50

# Indexing Settings
# Use LLM to enrich images/tables during indexing (slower but higher quality)
# Set to false for text-only indexing (10-100x faster, but misses visual content)
# PAPERQA_USE_ENRICHMENT=true
